{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "mnist.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NA_JUJxCtXNV"
      },
      "source": [
        "Import libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oXoBzQ6hs3ou"
      },
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tS7dJ_fvwXID"
      },
      "source": [
        "Define Hyperparameters"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NJFTGg9cwS2u"
      },
      "source": [
        "BATCH_SIZE = 32\n",
        "EPOCHS = 10\n",
        "VALIDATION_SPLIT = 0.2\n",
        "THRESHOLD = 0.95"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2um_jYq9tf8R"
      },
      "source": [
        "Fetch and preprocess MNIST data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oYmieCkbtTp1"
      },
      "source": [
        "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.mnist.load_data()\n",
        "\n",
        "# Rescale the images from [0,255] to the [0.0,1.0] range.\n",
        "x_train, x_test = x_train[..., np.newaxis]/255.0, x_test[..., np.newaxis]/255.0\n",
        "\n",
        "print(\"Number of original training examples:\", len(x_train))\n",
        "print(\"Number of original test examples:\", len(x_test))\n",
        "\n",
        "# Simulate unlabeled dataset\n",
        "labeled_index = np.random.choice(x_train.shape[0], int(x_train.shape[0] * 0.1), replace=False)\n",
        "labeled_mask = np.zeros(len(x_train), dtype=bool)\n",
        "labeled_mask[labeled_index] = 1\n",
        "labeled_x_train, labeled_y_train = x_train[labeled_mask], y_train[labeled_mask]\n",
        "unlabeled_x_train, unlabeled_y_train = x_train[~labeled_mask], y_train[~labeled_mask]\n",
        "\n",
        "print(\"Number of labeled training examples:\", len(labeled_x_train))\n",
        "print(\"Number of unlabeled training examples:\", len(unlabeled_x_train))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g3qD4uo6uXJq"
      },
      "source": [
        "Define model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uJhY6V1QuP2Z"
      },
      "source": [
        "model = tf.keras.models.Sequential([\n",
        "  tf.keras.layers.Conv2D(32, (3, 3), activation='relu', input_shape=(28, 28, 1)),\n",
        "  tf.keras.layers.MaxPooling2D((2, 2)),\n",
        "  tf.keras.layers.Conv2D(32, (3, 3), activation='relu'),\n",
        "  tf.keras.layers.MaxPooling2D((2, 2)),\n",
        "  tf.keras.layers.Flatten(),\n",
        "  tf.keras.layers.Dense(32,activation='relu'),\n",
        "  tf.keras.layers.Dense(10,activation='softmax')\n",
        "])\n",
        "\n",
        "model.compile(\n",
        "    optimizer=tf.keras.optimizers.Adam(0.001),\n",
        "    loss=tf.keras.losses.SparseCategoricalCrossentropy(),\n",
        "    metrics=[tf.keras.metrics.SparseCategoricalAccuracy()],\n",
        ")\n",
        "\n",
        "model.save_weights(\"./init\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m63YfG7mXmOc"
      },
      "source": [
        "Supervised (Baseline)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P8MWkfkRveNO"
      },
      "source": [
        "model.load_weights(\"./init\")\n",
        "best_loss = 1000\n",
        "for epoch in range(EPOCHS):\n",
        "  history = model.fit(labeled_x_train, labeled_y_train, validation_split=VALIDATION_SPLIT)\n",
        "  loss = history.history['val_loss'][0]\n",
        "  if loss < best_loss:\n",
        "    model.save_weights(\"./checkpoint\")\n",
        "    print(\"Saving checkpoint\")\n",
        "    best_loss = loss\n",
        "model.load_weights(\"./checkpoint\")\n",
        "hist = model.evaluate(x_test, y_test)\n",
        "supervised_loss, supervised_accuracy = hist[0], hist[1]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OPq1KPuYXvFl"
      },
      "source": [
        "Semi-Supervised"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8u8MchTLXpGl"
      },
      "source": [
        "semi_supervised_loss = []\n",
        "semi_supervised_accuracy = []\n",
        "\n",
        "while True:\n",
        "  best_loss = 1000\n",
        "  model.load_weights(\"./init\")\n",
        "\n",
        "  # Shuffle dataset\n",
        "  shuffler = np.random.permutation(len(labeled_y_train))\n",
        "  labeled_x_train, labeled_y_train = labeled_x_train[shuffler], labeled_y_train[shuffler]\n",
        "\n",
        "  # Supervised training\n",
        "  for epoch in range(EPOCHS):\n",
        "    model.evaluate(x_test,y_test)\n",
        "    history = model.fit(labeled_x_train, labeled_y_train, validation_split=VALIDATION_SPLIT)\n",
        "    loss = history.history['val_loss'][0]\n",
        "    if loss < best_loss:\n",
        "      model.save_weights(\"./checkpoint\")\n",
        "      print(\"Saving checkpoint\")\n",
        "      best_loss = loss\n",
        "\n",
        "  # Label unlabeled data\n",
        "  model.load_weights(\"./checkpoint\")\n",
        "  prediction = model.predict(unlabeled_x_train)\n",
        "\n",
        "  # Select and append \"confident\" unlabeled entries\n",
        "  mask = np.amax(prediction, axis=1) > THRESHOLD\n",
        "  new_x_train, new_y_train = unlabeled_x_train[mask], np.argmax(prediction[mask], axis=1)\n",
        "  labeled_x_train, labeled_y_train = np.vstack((labeled_x_train, new_x_train)), np.concatenate((labeled_y_train, new_y_train))\n",
        "  \n",
        "  # Evaluate\n",
        "  hist = model.evaluate(x_test,y_test)\n",
        "  semi_supervised_loss.append(hist[0])\n",
        "  semi_supervised_accuracy.append(hist[1])\n",
        "\n",
        "  # Exit training when not enough unlabeled data is appended\n",
        "  if np.sum(mask) < 20:\n",
        "    break\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y2vAFZUda7c0"
      },
      "source": [
        "Generating plots"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZveP9Ebn9lOr"
      },
      "source": [
        "plt.plot([0, 10], [supervised_loss, supervised_loss], color='b', linestyle='--', linewidth=2)\n",
        "plt.plot(semi_supervised_loss)\n",
        "plt.title('Loss')\n",
        "plt.ylabel('loss')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend([\"supervised\", \"semi-supervised\"], loc='upper left')\n",
        "plt.show()\n",
        "\n",
        "\n",
        "plt.plot([0, 10], [supervised_accuracy, supervised_accuracy], color='b', linestyle='--', linewidth=2)\n",
        "plt.plot(semi_supervised_accuracy)\n",
        "plt.title('Accuracy')\n",
        "plt.ylabel('accuracy')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend([\"supervised\", \"semi-supervised\"], loc='upper left')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}